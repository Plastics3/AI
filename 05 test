from sklearn.datasets import fetch_openml
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from PIL import Image, ImageOps
from DL3 import *

mnist = fetch_openml('mnist_784')
X, Y = mnist["data"], mnist["target"]


#1
X=np.array(X)
Y=np.array(Y)

X = X/255 -0.5
i = 12
dig3=X[12:13]
plt.imshow(X[i:i+1].reshape(28,28), cmap = matplotlib.cm.binary)
plt.axis("off")
plt.show()
print("Label is: '"+str(Y[i])+"'")

#2
Y_new = DLModel.to_one_hot(10,Y)
print("New label is:", Y_new[:,i])

#3
m = 60000
m_test = X.shape[0] - m
X_train, X_test = X[:m].T, X[m:].T
Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]
np.random.seed(111)
shuffle_index = np.random.permutation(m)
X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]
i = 12
plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)
plt.axis("off")
plt.show()
print(Y_train[:,i])

#4
layer64 = DLLayer("1",64,(784,),W_initialization = "Xaviar",learning_rate=0.1,activation="sigmoid",optimization="adaptive")
layer10 = DLLayer("1",10,(64,),W_initialization = "Xaviar",learning_rate=0.1,activation="softmax",optimization="adaptive")
model = DLModel("softmax")
model.add(layer64)
model.add(layer10)
model.compile("categorical_cross_entropy")

np.random.seed(1)
costs = model.train(X_train, Y_train, 200)
plt.plot(np.squeeze(costs))
plt.ylabel('cost')
plt.xlabel('iterations')
plt.title("Learning rate =" + str(0.1))
plt.show()
model.save_weights("digits")

print("Train:")
model.confusion_matrix(X_train, Y_train)
print("Test:")
#model.predict_softmax(X_test, Y_test) ???
